{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute code statistics history for NWB repositories\n",
    "\n",
    "**This notebook requires:**\n",
    "\n",
    " * CLOC command-line tool must be installed\n",
    " * GitPython (pip install GitPython)\n",
    " * matplotlib, numpy, pandas, yaml\n",
    " \n",
    "**This notebook computes:**\n",
    "\n",
    " * CLOC stats for the last commit on each day for all NWB code repositories\n",
    " * Plots the results for all repos\n",
    " \n",
    "**NOTE:** Computing these statistics is done by checking out all repositorires and then iterating over all commits in a repo, checking out the repo for the last commit on each day, and then computing CLOC. As such, computing these statistics is time consuming. The results can be cached to YAML for further processing and to save time when rerunning and editing the notebook. \n",
    "\n",
    "**NOTE:** Results in the ``output_dir`` may be erased and/or modified any time the script is rerun. If results need to preserved then copy any relevant files before rerunning the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import subprocess\n",
    "import git\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define script settings\n",
    "\n",
    "In this section we can update the main settings for the analyses in this notebook. Settings (e.g., color choices) specific to a plot appear with the corresponing plotting sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloc_path = \"/Users/oruebel/Devel/Libraries/cloc/cloc\"\n",
    "git_paths = {'PyNWB': \"https://github.com/NeurodataWithoutBorders/pynwb.git\", \n",
    "             'MatNWB': \"https://github.com/NeurodataWithoutBorders/matnwb.git\",\n",
    "             'NWBWidgets': \"https://github.com/NeurodataWithoutBorders/nwb-jupyter-widgets.git\",\n",
    "             'NWBInspector': \"https://github.com/NeurodataWithoutBorders/nwbinspector.git\",\n",
    "             'Hackathons': \"https://github.com/NeurodataWithoutBorders/nwb_hackathons.git\",\n",
    "             'NWB_Schema': \"https://github.com/NeurodataWithoutBorders/nwb-schema.git\",\n",
    "             'NWB_Schema_Language': \"https://github.com/NeurodataWithoutBorders/nwb-schema-language.git\",\n",
    "             'HDMF': 'https://github.com/hdmf-dev/hdmf.git',\n",
    "             'HDMF_Common_Schema': 'https://github.com/hdmf-dev/hdmf-common-schema.git',\n",
    "             'HDMF_DocUtils': 'https://github.com/hdmf-dev/hdmf-docutils.git',\n",
    "             # 'HDMF Schema Language' : https://github.com/hdmf-dev/hdmf-schema-language\n",
    "             'NDX_Template': 'https://github.com/nwb-extensions/ndx-template.git',\n",
    "             'NDX_Staged_Extensions': 'https://github.com/nwb-extensions/staged-extensions.git',\n",
    "             #'NDX Webservices': 'https://github.com/nwb-extensions/nwb-extensions-webservices.git',\n",
    "             'NDX_Catalog': 'https://github.com/nwb-extensions/nwb-extensions.github.io.git',\n",
    "             'NDX_Extension_Smithy': 'https://github.com/nwb-extensions/nwb-extensions-smithy',\n",
    "             'NWB_1.x_Matlab': 'https://github.com/NeurodataWithoutBorders/api-matlab.git',\n",
    "             'NWB_1.x_Python': 'https://github.com/NeurodataWithoutBorders/api-python.git'\n",
    "            }\n",
    "output_dir = os.path.join(os.getcwd(), 'temp_cloc_stats')\n",
    "source_dir = os.path.join(output_dir, 'src')\n",
    "cache_file_cloc = os.path.join(os.path.join(os.getcwd(), 'temp_cloc_stats'), 'cloc_stats.yaml')\n",
    "cache_file_commits = os.path.join(os.path.join(os.getcwd(), 'temp_cloc_stats'), 'commit_stats.yaml')\n",
    "load_cached_results = True  # load the cloc results from yaml\n",
    "cache_results = True  # save the cloc results to yaml\n",
    "show_NWB1 = False  # Remove NWB1 repos from the plots\n",
    "show_hackathons = False # Remove the hackathons from the plots\n",
    "save_figs = True   # Save the plots to file\n",
    "# Set all values before this date to 0 for HDMF 2019-03-13 coincides with the removal of HDMF from PyNWB with PR #850\n",
    "# and the release of HDMF 1.0. For the plotting 2019-03-13 is therefore a good date to start considering HDMF\n",
    "# stats to avoid duplication of code in statistics, even though the HDMF repo existed on GitHub already since\n",
    "# 2019-01-23T23:48:27Z, which could be alternatively considered as the start date. Older dates will include\n",
    "# code history carried over from PyNWB to HDMF. Set to None to consider the full history of HMDF but as mentioned,\n",
    "# this will lead to some duplicate counting of code before 2019-03-13\n",
    "hdmf_start_date = '2019-03-13'  \n",
    "# date when to declare the NWB 1.0 APIs as deprecated. The 3rd Hackathon was held on July 31 to August 1, 2017 at\n",
    "# Janelia Farm, in Ashburn, Virginia, which marks the date when NWB 2.0 was officially accepted as the \n",
    "# follow-up to NWB 1.0. NWB 1.0 as a project ended about 1 year before that.\n",
    "nwb1_depration_date = '2016-08-01'\n",
    "# NWB_Extension_Smithy is a fork with changes. We therefore should count only the sizes after the fork data\n",
    "# which based on https://api.github.com/repos/nwb-extensions/nwb-extensions-smithy is \"2019-04-25T20:56:02Z\",\n",
    "extension_smithy_start_date = '2019-04-25'\n",
    "# Select the repos and their order for the summary plot with the lines of code\n",
    "summary_plot_repos = [\n",
    "     'PyNWB', 'HDMF', 'MatNWB',\n",
    "     'NWB_Schema_Language', 'NWB_Schema', \n",
    "     'HDMF_Common_Schema', 'HDMF_DocUtils',\n",
    "     'NDX_Catalog', 'NDX_Template', 'NDX_Staged_Extensions', 'NDX_Extension_Smithy',\n",
    "     'NWBWidgets', 'NWBInspector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions used to interact with git and compute CLOC stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outdirs(output_dir, source_dir):\n",
    "    \"\"\"\n",
    "    Delete the output directory and all its contents and create a new clean directory.\n",
    "    \n",
    "    :returns: A tuple of two strings with the output_dir and source_dir for git sources\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(source_dir)\n",
    "    return output_dir, source_dir\n",
    "\n",
    "def clone_repos(repos, source_dir):\n",
    "    \"\"\"\n",
    "    Clone all of the given repositories. \n",
    "    \n",
    "    :param repos: Dict where the keys are the names of the repos and the\n",
    "                  values are the git source path to clone\n",
    "    :param source_dir: Directory where all the git repos should be cloned to.\n",
    "                  Each repo will be cloned into a subdirectory in source_dir\n",
    "                  that is named after the corresponding key in the repos dict.\n",
    "    :returns: Dict where the keys are the same as in repos but the values\n",
    "              are instances of git.repo.base.Repo pointing to the corresponding\n",
    "              git repository.\n",
    "    \"\"\"\n",
    "    git_repos = {}\n",
    "    for k, v in repos.items():\n",
    "        print(\"Cloning: %s\" % k)\n",
    "        git_repos[k] = git.Repo.clone_from(v, os.path.join(source_dir, k))\n",
    "    return git_repos\n",
    "\n",
    "def run_cloc(cloc_path, src_dir, out_file):\n",
    "    \"\"\"\n",
    "    Run CLOC on the given srcdir, save the results to outdir, and return the parsed\n",
    "    results.\n",
    "    \"\"\"\n",
    "    command = \"%s --yaml --report-file=%s %s\" % (cloc_path, out_file, src_dir)\n",
    "    os.system(command)\n",
    "    with open(out_file) as f:\n",
    "        res = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return res\n",
    "\n",
    "def git_repo_stats(repo, cloc_path, output_dir):\n",
    "    \"\"\"\n",
    "    :param repo: The git repository to process\n",
    "    :type repo: git.repo.base.Repo\n",
    "    \n",
    "    :returns: List of dicts with information about all commits. The list\n",
    "              is sorted in time from most current [0] to oldest [-1]\n",
    "    \"\"\"\n",
    "    # Get hexsha and data of all commits\n",
    "    commit_stats = []\n",
    "    # Commits are sorted in time from newest to oldest\n",
    "    for commit in repo.iter_commits():\n",
    "        commit_stats.append(\n",
    "            {'time': time.asctime(time.gmtime(commit.committed_date)),\n",
    "             'hexsha': commit.hexsha,\n",
    "             'author': commit.author.name,\n",
    "             'committer': commit.committer.name,\n",
    "             'summary': commit.summary,\n",
    "             'commit': commit})\n",
    "    # iterate through all the commits in order and compute the cloc stats\n",
    "    cloc_stats = []\n",
    "    for commit in commit_stats:\n",
    "        date = time.strftime(\"%d %b %Y\", time.gmtime(commit['commit'].committed_date))\n",
    "        # Run cloc only for the last commit on each day\n",
    "        if len(cloc_stats) == 0 or date != cloc_stats[-1]['date']:\n",
    "            cloc_res = {'hexsha': commit['hexsha'], 'date': date, 'time': commit['time']}\n",
    "            repo.git.checkout(commit['hexsha'])\n",
    "            cloc_yaml = os.path.join(\n",
    "                output_dir, \n",
    "                \"%s.yaml\" % os.path.basename(repo.working_dir))\n",
    "                #\"%s_%s.yaml\" % (os.path.dirname(repo.working_dir), commit['hexsha']))\n",
    "            cloc_res['cloc'] = run_cloc(\n",
    "                cloc_path=cloc_path, \n",
    "                src_dir=repo.working_dir, \n",
    "                out_file=cloc_yaml)\n",
    "            os.remove(cloc_yaml) # Remove the yaml file, we don't need it\n",
    "            cloc_stats.append(cloc_res)\n",
    "        # drop the commit from the dict to make sure we can save things in YAML\n",
    "        commit.pop('commit', None)\n",
    "    return commit_stats, cloc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the code statistics for all repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached results if available\n",
    "if load_cached_results and os.path.exists(cache_file_cloc) and os.path.exists(cache_file_commits):\n",
    "    print(\"Loading cached results: %s\" % cache_file_cloc)\n",
    "    with open(cache_file_cloc) as f:\n",
    "        cloc_stats = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(\"Loading cached results: %s\" % cache_file_commits)\n",
    "    with open(cache_file_commits) as f:\n",
    "        commit_stats = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# Compute the results if not cached\n",
    "else: \n",
    "    # Clean and create output directory\n",
    "    clean_outdirs(output_dir=output_dir, \n",
    "                  source_dir=source_dir)\n",
    "    # Clone all repos\n",
    "    print(\"Cloning all repos...\")\n",
    "    git_repos = clone_repos(repos=git_paths, source_dir=source_dir)\n",
    "    # Compute CLOC and Commit statistics for all repos\n",
    "    commit_stats = {}\n",
    "    cloc_stats = {}\n",
    "    for name, repo in git_repos.items():\n",
    "        print(\"Compute CLOC stats: %s\" % name)\n",
    "        commit_res, cloc_res = git_repo_stats(\n",
    "            repo, \n",
    "            cloc_path=cloc_path, \n",
    "            output_dir=output_dir)\n",
    "        commit_stats[name] = commit_res\n",
    "        cloc_stats[name] = cloc_res\n",
    "    # Cache the results if requested\n",
    "    if cache_results:\n",
    "        print(\"Caching results: %s\" % cache_file_cloc)\n",
    "        with open(cache_file_cloc, 'w') as outfile:\n",
    "            yaml.dump(cloc_stats, outfile)\n",
    "        print(\"Caching results: %s\" % cache_file_commits)\n",
    "        with open(cache_file_commits, 'w') as outfile:\n",
    "            yaml.dump(commit_stats, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of the lines of code across all NWB repos\n",
    "### 4.1. Compile summary of LOC across repos by catagories: `blank`, `comment`, `code`, `nFiles`, `size`\n",
    "\n",
    "The goal is to align and expand results from all repos so that we can plot them together. Here we create a continoues date range and expand the results from all repos to align with our common time axis. For dates where no new CLOC stats are recorded for a repo, the statistics from the previous time are carried forward to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our reference date range depending on whether we include NWB 1 in the plots or not\n",
    "if show_NWB1:\n",
    "    date_range = pd.date_range(start=cloc_stats['NWB_1.x_Matlab'][-1]['date'], end=time.strftime(\"%d %b %Y\", time.localtime()), freq=\"D\")\n",
    "else:\n",
    "    date_range = pd.date_range(start=cloc_stats['PyNWB'][-1]['date'], end=time.strftime(\"%d %b %Y\", time.localtime()), freq=\"D\")\n",
    "\n",
    "# Align and expand our results\n",
    "repo_sizes_aligned = {}\n",
    "repo_blanks_aligned = {}\n",
    "repo_codes_aligned = {}\n",
    "repo_comments_aligned = {}\n",
    "repo_nfiles_aligned = {}\n",
    "# Iterate through all repos and organize the size stats for the given date_range\n",
    "for k, v in cloc_stats.items():\n",
    "    # Dates and CLOC size for the current repo\n",
    "    curr_dates = pd.pandas.DatetimeIndex([cloc_entry['date'] for cloc_entry in v])[::-1]\n",
    "    curr_sizes = [np.sum([v for k, v in cloc_entry['cloc']['SUM'].items() if k != 'nFiles']) for cloc_entry in v][::-1]\n",
    "    curr_blanks = [cloc_entry['cloc']['SUM']['blank'] for cloc_entry in v][::-1]\n",
    "    curr_codes = [cloc_entry['cloc']['SUM']['code'] for cloc_entry in v][::-1]\n",
    "    curr_comments = [cloc_entry['cloc']['SUM']['comment'] for cloc_entry in v][::-1]\n",
    "    curr_nfiles = [cloc_entry['cloc']['SUM']['nFiles'] for cloc_entry in v][::-1]\n",
    "    \n",
    "    # Expand the data so we carry forward values for dates where the repo has not changed\n",
    "    curr_index = 0\n",
    "    curr_val_sizes = 0\n",
    "    curr_val_blanks = 0\n",
    "    curr_val_codes = 0\n",
    "    curr_val_comments = 0\n",
    "    curr_val_nfiles = 0\n",
    "    expanded_sizes = []\n",
    "    expanded_blanks = []\n",
    "    expanded_codes = []\n",
    "    expanded_comments = []\n",
    "    expanded_nfiles = []\n",
    "    # If our start date of the repo is before the start of our date_range,\n",
    "    # then we need to search for the approbriate start values and index\n",
    "    # as the repo has a valid state prior to the range we are looking at\n",
    "    if date_range[0] > curr_dates[0]:\n",
    "        if date_range[0] > curr_dates[-1]:\n",
    "            curr_index = len(curr_dates) -1\n",
    "        else:\n",
    "            for di ,d in enumerate(curr_dates):\n",
    "                if d > date_range[0]:\n",
    "                    curr_index = di - 1\n",
    "                    break\n",
    "        curr_val_sizes = curr_sizes[curr_index]\n",
    "        curr_val_blanks = curr_blanks[curr_index]\n",
    "        curr_val_codes = curr_codes[curr_index]\n",
    "        curr_val_comments = curr_comments[curr_index]\n",
    "        curr_val_nfiles = curr_nfiles[curr_index]\n",
    "                \n",
    "    # Compute all the sizes\n",
    "    for d in date_range:\n",
    "        # If we found a matching date, then update the results\n",
    "        # Else we'll carry-forward the previous value since the\n",
    "        # repo has not changed\n",
    "        if d == curr_dates[curr_index]:\n",
    "            curr_val_sizes = curr_sizes[curr_index]\n",
    "            curr_val_blanks = curr_blanks[curr_index]\n",
    "            curr_val_codes = curr_codes[curr_index]\n",
    "            curr_val_comments = curr_comments[curr_index]\n",
    "            curr_val_nfiles = curr_nfiles[curr_index]\n",
    "            if curr_index < (len(curr_dates) -1):\n",
    "                curr_index += 1\n",
    "        # Append the approbriate value for the current data d\n",
    "        expanded_sizes.append(curr_val_sizes)\n",
    "        expanded_blanks.append(curr_val_blanks)\n",
    "        expanded_codes.append(curr_val_codes)\n",
    "        expanded_comments.append(curr_val_comments)\n",
    "        expanded_nfiles.append(curr_val_nfiles)\n",
    "    # Save the expanded results for the current repo k\n",
    "    repo_sizes_aligned[k] = np.asarray(expanded_sizes)\n",
    "    repo_blanks_aligned[k] = np.asarray(expanded_blanks)\n",
    "    repo_codes_aligned[k] = np.asarray(expanded_codes)\n",
    "    repo_comments_aligned[k] = np.asarray(expanded_comments)\n",
    "    repo_nfiles_aligned[k] = np.asarray(expanded_nfiles)\n",
    "        \n",
    "# Convert results to Pandas\n",
    "repo_sizes_algined_df = pd.DataFrame.from_dict(repo_sizes_aligned)\n",
    "repo_sizes_algined_df.index = date_range\n",
    "\n",
    "# Clean up results to mark start date of HDMF\n",
    "if hdmf_start_date is not None:\n",
    "    # Set all LOC values prior to the given data to 0\n",
    "    repo_sizes_algined_df['HDMF'][:hdmf_start_date] = 0\n",
    "# Clean up results to mark start date for the extension smithy\n",
    "if extension_smithy_start_date is not None:\n",
    "     # Set all LOC values prior to the given data to 0\n",
    "    repo_sizes_algined_df['NDX_Extension_Smithy'][:extension_smithy_start_date] = 0\n",
    "# Clean up results to remove NWB 1.0 software from the graph after 1.0 was deprecated\n",
    "if nwb1_depration_date is not None:\n",
    "    repo_sizes_algined_df['NWB_1.x_Matlab'][nwb1_depration_date:] = 0\n",
    "    repo_sizes_algined_df['NWB_1.x_Python'][nwb1_depration_date:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Plot summary of the lines of code across all NWB repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenly_spaced_interval = np.linspace(0, 1, len(summary_plot_repos))\n",
    "#colors = [cm.tab20(x) for x in evenly_spaced_interval]\n",
    "colors = [cm.Paired(x) for x in evenly_spaced_interval]\n",
    "# mix up colors so that neighbouring areas have more dissimilar colors\n",
    "colors = [c for i, c in enumerate(colors) if i % 2 == 0] + [c for i, c in enumerate(colors) if i % 2 == 1]\n",
    "repo_sizes_algined_df[summary_plot_repos].plot.area(\n",
    "    figsize=(18,10), \n",
    "    stacked=True, \n",
    "    linewidth=0,\n",
    "    fontsize=16, \n",
    "    color=colors)\n",
    "plt.legend(loc=2, prop={'size': 16})\n",
    "plt.ylabel('Lines of Code (CLOC)', fontsize=16)\n",
    "plt.grid(color='black', linestyle='--', linewidth=0.7, axis='both')\n",
    "plt.title('NWB Code Repository Sizes', fontsize=20)\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    plt.savefig(os.path.join(output_dir, 'nwb_repo_sizes_all.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Plot per-repo total lines of code statistics broken down by: code, blank, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in git_paths.keys():\n",
    "    curr_df = pd.DataFrame.from_dict({'code': repo_codes_aligned[k], \n",
    "                                      'blank': repo_blanks_aligned[k], \n",
    "                                      'comment': repo_comments_aligned[k]})\n",
    "    curr_df.index = date_range\n",
    "    curr_df.plot.area(\n",
    "        figsize=(18,10), \n",
    "        stacked=True, \n",
    "        linewidth=0, \n",
    "        fontsize=16)\n",
    "    plt.legend(loc=2, prop={'size': 16})\n",
    "    plt.ylabel('Lines of Code (CLOC)', fontsize=16)\n",
    "    plt.grid(color='black', linestyle='--', linewidth=0.7, axis='both')\n",
    "    plt.title(\"Lines of Code: %s\" % k, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join(output_dir, '%s_loc.pdf' % k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-repo total lines of code statistics broken down by language type\n",
    "### 5.1. Compute the per-repo language statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all repos \n",
    "ignore_lang = ['SUM', 'header']\n",
    "languages_used_all = np.unique([lang for v in cloc_stats.values() for cl in v for lang in cl['cloc'].keys() if lang not in ignore_lang])\n",
    "per_repo_lang_stats = {}\n",
    "for k, v in cloc_stats.items():\n",
    "    # languages used in the current repo\n",
    "    languages_used = np.unique([lang for cl in v for lang in cl['cloc'].keys() if lang not in ignore_lang])\n",
    "    # linear range of dates across the lifetime of this repo\n",
    "    date_range_used = pd.date_range(start=cloc_stats[k][-1]['date'], end=time.strftime(\"%d %b %Y\", time.localtime()), freq=\"D\")\n",
    "    curr_index = 0  # start index in the CLOC data available for the repo\n",
    "    curr_values = {l: 0 for l in languages_used}  # current values to be used\n",
    "    curr_dates = pd.pandas.DatetimeIndex([cloc_entry['date'] for cloc_entry in v])[::-1] # dates available in the repo\n",
    "    curr_stats = {lang: [] for lang in languages_used}\n",
    "    # iterate through all date values and set the repo counts\n",
    "    for d in date_range_used:\n",
    "        # If we found a matching date, then update the results\n",
    "        # Else we'll carry-forward the previous value since the\n",
    "        # repo has not changed\n",
    "        if d == curr_dates[curr_index]:\n",
    "            # Update the current values to report until we find curr_dates[curr_index+1]\n",
    "            for lang, val in v[curr_index]['cloc'].items():\n",
    "                if lang in curr_values:  # e.g., SUM is being ignored\n",
    "                    curr_values[lang] = val['blank'] + val['code'] + val['code']\n",
    "            # Move to the next date in the repo\n",
    "            if curr_index < (len(curr_dates) -1):\n",
    "                curr_index += 1\n",
    "        # Copy the current values into our curr_stats dict\n",
    "        for cl, cv in curr_values.items():\n",
    "            curr_stats[cl].append(cv)\n",
    "    # Now that we have our stats lets convert them to pandas \n",
    "    per_repo_lang_stats[k] = pd.DataFrame.from_dict(curr_stats)\n",
    "    per_repo_lang_stats[k].index = date_range_used[::-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Plot the per-repo total lines of code statistics broken down by language type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique colors per language so we can be consistent across plots\n",
    "evenly_spaced_interval = np.linspace(0, 1, len(languages_used_all))\n",
    "language_colors = {languages_used_all[i]:cm.jet(x) #tab20(x) \n",
    "                   for i, x in enumerate(evenly_spaced_interval)}\n",
    "# Iterate through all repos and plot the per-language LOC stats for each repo\n",
    "for k, v in per_repo_lang_stats.items():\n",
    "    v.plot.area( \n",
    "        figsize=(18,10), \n",
    "        stacked=True, \n",
    "        linewidth=0, \n",
    "        fontsize=16,\n",
    "        color = [language_colors[l] for l in v.columns]\n",
    "    )\n",
    "    plt.legend(loc=2, prop={'size': 16})\n",
    "    plt.ylabel('Lines of Code (CLOC)', fontsize=16)\n",
    "    plt.grid(color='black', linestyle='--', linewidth=0.7, axis='both')\n",
    "    plt.title(\"Lines of Code: %s\" % k, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join(output_dir, '%s_language_loc.pdf' % k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
