{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute code statistics history for NWB repositories\n",
    "\n",
    "**This notebook requires:**\n",
    "\n",
    " * CLOC command-line tool must be installed\n",
    " * GitPython (pip install GitPython)\n",
    " * matplotlib, numpy, pandas, yaml\n",
    " \n",
    "**This notebook computes:**\n",
    "\n",
    " * CLOC stats for the last commit on each day for all NWB code repositories\n",
    " * Plots the results for all repos\n",
    " \n",
    "**NOTE:** Computing these statistics is done by checking out all repositorires and then iterating over all commits in a repo, checking out the repo for the last commit on each day, and then computing CLOC. As such, computing these statistics is time consuming. The results can be cached to YAML for further processing and to save time when rerunning and editing the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import subprocess\n",
    "import git\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def clean_outdirs(output_dir, source_dir):\n",
    "    \"\"\"\n",
    "    Delete the output directory and all its contents and create a new clean directory.\n",
    "    \n",
    "    :returns: A tuple of two strings with the output_dir and source_dir for git sources\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(source_dir)\n",
    "    return output_dir, source_dir\n",
    "\n",
    "def clone_repos(repos, source_dir):\n",
    "    \"\"\"\n",
    "    Clone all of the given repositories. \n",
    "    \n",
    "    :param repos: Dict where the keys are the names of the repos and the\n",
    "                  values are the git source path to clone\n",
    "    :param source_dir: Directory where all the git repos should be cloned to.\n",
    "                  Each repo will be cloned into a subdirectory in source_dir\n",
    "                  that is named after the corresponding key in the repos dict.\n",
    "    :returns: Dict where the keys are the same as in repos but the values\n",
    "              are instances of git.repo.base.Repo pointing to the corresponding\n",
    "              git repository.\n",
    "    \"\"\"\n",
    "    git_repos = {}\n",
    "    for k, v in repos.items():\n",
    "        print(\"Cloning: %s\" % k)\n",
    "        git_repos[k] = git.Repo.clone_from(v, os.path.join(source_dir, k))\n",
    "    return git_repos\n",
    "\n",
    "def run_cloc(cloc_path, src_dir, out_file):\n",
    "    \"\"\"\n",
    "    Run CLOC on the given srcdir, save the results to outdir, and return the parsed\n",
    "    results.\n",
    "    \"\"\"\n",
    "    command = \"%s --yaml --report-file=%s %s\" % (cloc_path, out_file, src_dir)\n",
    "    os.system(command)\n",
    "    with open(out_file) as f:\n",
    "        res = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return res\n",
    "\n",
    "def git_repo_stats(repo, cloc_path, output_dir):\n",
    "    \"\"\"\n",
    "    :param repo: The git repository to process\n",
    "    :type repo: git.repo.base.Repo\n",
    "    \n",
    "    :returns: List of dicts with information about all commits. The list\n",
    "              is sorted in time from most current [0] to oldest [-1]\n",
    "    \"\"\"\n",
    "    # Get hexsha and data of all commits\n",
    "    commit_stats = []\n",
    "    # Commits are sorted in time from newest to oldest\n",
    "    for commit in repo.iter_commits():\n",
    "        commit_stats.append(\n",
    "            {'time': time.asctime(time.gmtime(commit.committed_date)),\n",
    "             'hexsha': commit.hexsha,\n",
    "             'author': commit.author.name,\n",
    "             'committer': commit.committer.name,\n",
    "             'summary': commit.summary,\n",
    "             'commit': commit})\n",
    "    # iterate through all the commits in order and compute the cloc stats\n",
    "    cloc_stats = []\n",
    "    for commit in commit_stats:\n",
    "        date = d = time.strftime(\"%d %b %Y\", time.gmtime(commit['commit'].committed_date))\n",
    "        # Run cloc only for the last commit on each day\n",
    "        if len(cloc_stats) == 0 or d != cloc_stats[-1]['date']:\n",
    "            cloc_res = {'hexsha': commit['hexsha'], 'date': date, 'time': commit['time']}\n",
    "            repo.git.checkout(commit['hexsha'])\n",
    "            cloc_yaml = os.path.join(\n",
    "                output_dir, \n",
    "                \"%s.yaml\" % os.path.basename(repo.working_dir))\n",
    "                #\"%s_%s.yaml\" % (os.path.dirname(repo.working_dir), commit['hexsha']))\n",
    "            cloc_res['cloc'] = run_cloc(\n",
    "                cloc_path=cloc_path, \n",
    "                src_dir=repo.working_dir, \n",
    "                out_file=cloc_yaml)\n",
    "            os.remove(cloc_yaml) # Remove the yaml file, we don't need it\n",
    "            cloc_stats.append(cloc_res)\n",
    "    return commit_stats, cloc_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define script settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloc_path = \"/Users/oruebel/Devel/Libraries/cloc/cloc\"\n",
    "git_paths = {'PyNWB': \"https://github.com/NeurodataWithoutBorders/pynwb.git\", \n",
    "             'MatNWB': \"https://github.com/NeurodataWithoutBorders/matnwb.git\",\n",
    "             'NWBWidgets': \"https://github.com/NeurodataWithoutBorders/nwb-jupyter-widgets.git\",\n",
    "             # 'Hackathons': \"https://github.com/NeurodataWithoutBorders/nwb_hackathons.git\",\n",
    "             'NWB_Schema': \"https://github.com/NeurodataWithoutBorders/nwb-schema.git\",\n",
    "             'NWB_Schema_Language': \"https://github.com/NeurodataWithoutBorders/nwb-schema-language.git\",\n",
    "             'HDMF': 'https://github.com/hdmf-dev/hdmf.git',\n",
    "             'HDMF_Common_Schema': 'https://github.com/hdmf-dev/hdmf-common-schema.git',\n",
    "             'HDMF_DocUtils': 'https://github.com/hdmf-dev/hdmf-docutils.git',\n",
    "             # 'HDMF Schema Language' : https://github.com/hdmf-dev/hdmf-schema-language\n",
    "             'NDX_Template': 'https://github.com/nwb-extensions/ndx-template.git',\n",
    "             'NDX_Staged_Extensions': 'https://github.com/nwb-extensions/staged-extensions.git',\n",
    "             #'NDX Webservices': 'https://github.com/nwb-extensions/nwb-extensions-webservices.git',\n",
    "             'NDX_Catalog': 'https://github.com/nwb-extensions/nwb-extensions.github.io.git',\n",
    "             #'NDX Extension Smithy': 'https://github.com/nwb-extensions/nwb-extensions-smithy'\n",
    "             'NWB_1.x_Matlab': 'https://github.com/NeurodataWithoutBorders/api-matlab.git',\n",
    "             'NWB_1.x_Python': 'https://github.com/NeurodataWithoutBorders/api-python.git'\n",
    "            }\n",
    "output_dir = os.path.join(os.getcwd(), 'temp_cloc_stats')\n",
    "source_dir = os.path.join(output_dir, 'src')\n",
    "cache_file = os.path.join(os.path.join(os.getcwd(), 'temp_cloc_stats'), 'cloc_stats.yaml')\n",
    "load_cached_results = True  # load the cloc results from yaml\n",
    "cache_results = True  # save the cloc results to yaml\n",
    "show_NWB1 = False  # Remove NWB1 repos from the plots\n",
    "save_figs = True   # Save the plots to file\n",
    "# Set all values before this date to 0 for HDMF 2019-03-13 coincides with the removal of HDMF from PyNWB with PR #850\n",
    "# and the release of HDMF 1.0. For the plotting 2019-03-13 is therefore a good date to start considering HDMF\n",
    "# stats to avoid duplication of code in statistics, even though the HDMF repo existed on GitHub already since\n",
    "# 2019-01-23T23:48:27Z, which could be alternatively considered as the start date. Older dates will include\n",
    "# code history carried over from PyNWB to HDMF. Set to None to consider the full history of HMDF but as mentioned,\n",
    "# this will lead to some duplicate counting of code before 2019-03-13\n",
    "hdmf_start_date = '2019-03-13'  \n",
    "# date when to declare the NWB 1.0 APIs as deprecated. The 3rd Hackathon was held on July 31 to August 1, 2017 at\n",
    "# Janelia Farm, in Ashburn, Virginia, which marks the date when NWB 2.0 was officially accepted as the \n",
    "# follow-up to NWB 1.0. NWB 1.0 as a project ended about 1 year before that.\n",
    "nwb1_depration_date = '2016-08-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the code statistics for all repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached results if available\n",
    "if load_cached_results and os.path.exists(cache_file):\n",
    "    print(\"Loading cached results: %s\" % cache_file)\n",
    "    with open(cache_file) as f:\n",
    "        cloc_stats = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# Compute the results if not cached\n",
    "else: \n",
    "    # Clean and create output directory\n",
    "    output_dir, source_dir = clean_outdirs()\n",
    "    # Clone all repos\n",
    "    print(\"Cloning all repos...\")\n",
    "    git_repos = clone_repos(repos=git_paths, source_dir=source_dir)\n",
    "    # Compute CLOC and Commit statistics for all repos\n",
    "    commit_stats = {}\n",
    "    cloc_stats = {}\n",
    "    for name, repo in git_repos.items():\n",
    "        print(\"Compute CLOC stats: %s\" % name)\n",
    "        commit_res, cloc_res = git_repo_stats(\n",
    "            repo, \n",
    "            cloc_path=cloc_path, \n",
    "            output_dir=output_dir)\n",
    "        commit_stats[name] = commit_res\n",
    "        cloc_stats[name] = cloc_res\n",
    "    # Cache the results if requested\n",
    "    if cache_results:\n",
    "        print(\"Caching results: %s\" % cache_file)\n",
    "        with open(cache_file, 'w') as outfile:\n",
    "            yaml.dump(cloc_stats, outfile),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align and expand results from all repos\n",
    "\n",
    "Here we create a continoues date range and expand the results from all repos to align with our common time axis. For dates where no new CLOC stats are recorded for a repo, the statistics from the previous time are carried forward to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our reference date range depending on whether we include NWB 1 in the plots or not\n",
    "if show_NWB1:\n",
    "    date_range = pd.date_range(start=cloc_stats['NWB_1.x_Matlab'][-1]['date'], end=time.strftime(\"%d %b %Y\", time.localtime()), freq=\"D\")\n",
    "else:\n",
    "    date_range = pd.date_range(start=cloc_stats['PyNWB'][-1]['date'], end=time.strftime(\"%d %b %Y\", time.localtime()), freq=\"D\")\n",
    "\n",
    "\n",
    "# Align and expand our results\n",
    "repo_sizes_aligned = {}\n",
    "repo_blanks_aligned = {}\n",
    "repo_codes_aligned = {}\n",
    "repo_comments_aligned = {}\n",
    "repo_nfiles_aligned = {}\n",
    "\n",
    "for k, v in cloc_stats.items():\n",
    "    # Dates and CLOC size for the current repo\n",
    "    curr_dates = pd.pandas.DatetimeIndex([cloc_entry['date'] for cloc_entry in v])[::-1]\n",
    "    curr_sizes = [np.sum([v for k, v in cloc_entry['cloc']['SUM'].items() if k != 'nFiles']) for cloc_entry in v][::-1]\n",
    "    curr_blanks = [cloc_entry['cloc']['SUM']['blank'] for cloc_entry in v][::-1]\n",
    "    curr_codes = [cloc_entry['cloc']['SUM']['code'] for cloc_entry in v][::-1]\n",
    "    curr_comments = [cloc_entry['cloc']['SUM']['comment'] for cloc_entry in v][::-1]\n",
    "    curr_nfiles = [cloc_entry['cloc']['SUM']['nFiles'] for cloc_entry in v][::-1]\n",
    "    \n",
    "    # Expand the data so we carry forward values for dates where the repo has not changed\n",
    "    curr_index = 0\n",
    "    curr_val_sizes = 0\n",
    "    curr_val_blanks = 0\n",
    "    curr_val_codes = 0\n",
    "    curr_val_comments = 0\n",
    "    curr_val_nfiles = 0\n",
    "    expanded_sizes = []\n",
    "    expanded_blanks = []\n",
    "    expanded_codes = []\n",
    "    expanded_comments = []\n",
    "    expanded_nfiles = []\n",
    "    for d in date_range:\n",
    "        if d == curr_dates[curr_index]:\n",
    "            curr_val_sizes = curr_sizes[curr_index]\n",
    "            curr_val_blanks = curr_blanks[curr_index]\n",
    "            curr_val_codes = curr_codes[curr_index]\n",
    "            curr_val_comments = curr_comments[curr_index]\n",
    "            curr_val_nfiles = curr_nfiles[curr_index]\n",
    "            if curr_index < (len(curr_dates) -1):\n",
    "                curr_index += 1\n",
    "        expanded_sizes.append(curr_val_sizes)\n",
    "        expanded_blanks.append(curr_val_blanks)\n",
    "        expanded_codes.append(curr_val_codes)\n",
    "        expanded_comments.append(curr_val_comments)\n",
    "        expanded_nfiles.append(curr_val_nfiles)\n",
    "    repo_sizes_aligned[k] = np.asarray(expanded_sizes)\n",
    "    repo_blanks_aligned[k] = np.asarray(expanded_blanks)\n",
    "    repo_codes_aligned[k] = np.asarray(expanded_codes)\n",
    "    repo_comments_aligned[k] = np.asarray(expanded_comments)\n",
    "    repo_nfiles_aligned[k] = np.asarray(expanded_nfiles)\n",
    "    \n",
    "    \n",
    "# Convert results to Pandas\n",
    "repo_sizes_algined_df = pd.DataFrame.from_dict(repo_sizes_aligned)\n",
    "repo_sizes_algined_df.index = date_range\n",
    "\n",
    "# Clean up results to mark start date of HDMF\n",
    "if hdmf_start_date is not None:\n",
    "    # Set all LOC values prior to the given data to 0\n",
    "    repo_sizes_algined_df['HDMF'][:hdmf_start_date] = 0\n",
    "# Clean up results to remove NWB 1.0 software from the graph after 1.0 was deprecated\n",
    "if nwb1_depration_date is not None:\n",
    "    repo_sizes_algined_df['NWB_1.x_Matlab'][nwb1_depration_date:] = 0\n",
    "    repo_sizes_algined_df['NWB_1.x_Python'][nwb1_depration_date:] = 0\n",
    "# Remove NWB 1 from the datafram if necessary\n",
    "if not show_NWB1:\n",
    "    repo_sizes_algined_df = repo_sizes_algined_df.drop('NWB_1.x_Matlab', axis=1).drop('NWB_1.x_Python', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the lines of code statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenly_spaced_interval = np.linspace(0, 1, len(repo_sizes_algined_df.columns))\n",
    "colors = [cm.tab20(x) for x in evenly_spaced_interval]\n",
    "colors = [c for i, c in enumerate(colors) if i % 2 == 0] + [c for i, c in enumerate(colors) if i % 2 == 1]\n",
    "#plot_order = repo_sizes_algined_df.columns[::-1]\n",
    "plot_order = ['PyNWB', 'HDMF', 'MatNWB',\n",
    "              'NWB_Schema_Language', 'NWB_Schema', \n",
    "              'HDMF_Common_Schema', 'HDMF_DocUtils',\n",
    "              'NDX_Catalog', 'NDX_Template', 'NDX_Staged_Extensions',\n",
    "              'NWBWidgets']\n",
    "if show_NWB1:\n",
    "    plot_order += ['NWB_1.x_Matlab', 'NWB_1.x_Python']\n",
    "\n",
    "repo_sizes_algined_df[plot_order].plot.area(\n",
    "    figsize=(18,10), \n",
    "    stacked=True, \n",
    "    linewidth=0, \n",
    "    fontsize=16, \n",
    "    color=colors)\n",
    "plt.legend(loc=2, prop={'size': 16})\n",
    "plt.ylabel('Lines of Code (CLOC)', fontsize=16)\n",
    "plt.grid(color='black', linestyle='--', linewidth=0.7, axis='both')\n",
    "plt.title('NWB Code Repository Sizes', fontsize=20)\n",
    "plt.tight_layout()\n",
    "if save_figs:\n",
    "    plt.savefig(os.path.join(output_dir, 'nwb_repo_sizes_all.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in git_paths.keys():\n",
    "    if not show_NWB1:\n",
    "        if k in ['NWB_1.x_Matlab', 'NWB_1.x_Python']:\n",
    "            continue\n",
    "    curr_df = pd.DataFrame.from_dict({'code': repo_codes_aligned[k], \n",
    "                                      'blank': repo_blanks_aligned[k], \n",
    "                                      'comment': repo_comments_aligned[k]})\n",
    "    curr_df.index = date_range\n",
    "    curr_df.plot.area(\n",
    "        figsize=(18,10), \n",
    "        stacked=True, \n",
    "        linewidth=0, \n",
    "        fontsize=16)\n",
    "    plt.legend(loc=2, prop={'size': 16})\n",
    "    plt.ylabel('Lines of Code (CLOC)', fontsize=16)\n",
    "    plt.grid(color='black', linestyle='--', linewidth=0.7, axis='both')\n",
    "    plt.title(\"Lines of Code: %s\" % k, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join(output_dir, '%s_loc.pdf' % k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
