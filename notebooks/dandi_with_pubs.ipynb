{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb453e-fcf4-407b-843c-1e362054503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bisect import bisect\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f7bec-3fcb-4a30-94a5-6649bb6420fc",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f2f77-3d88-4759-b2ea-eea8b88c8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes pretty-printing\n",
    "UNITS_MAPPING = [\n",
    "    (1<<50, ' PB'),\n",
    "    (1<<40, ' TB'),\n",
    "    (1<<30, ' GB'),\n",
    "    (1<<20, ' MB'),\n",
    "    (1<<10, ' KB'),\n",
    "    (1, (' byte', ' bytes')),\n",
    "]\n",
    "\n",
    "\n",
    "def pretty_size(bytes, units=UNITS_MAPPING):\n",
    "    \"\"\"\n",
    "    Get human-readable file sizes.\n",
    "    simplified version of https://pypi.python.org/pypi/hurry.filesize/\n",
    "    \"\"\"\n",
    "    for factor, suffix in units:\n",
    "        if bytes >= factor:\n",
    "            break\n",
    "    amount = int(bytes / factor)\n",
    "\n",
    "    if isinstance(suffix, tuple):\n",
    "        singular, multiple = suffix\n",
    "        if amount == 1:\n",
    "            suffix = singular\n",
    "        else:\n",
    "            suffix = multiple\n",
    "    return str(amount) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf25915-bb95-4ff6-b6e5-709c3f9056bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_nwb(metadata):\n",
    "    return any(\n",
    "        x['identifier'] == 'RRID:SCR_015242'\n",
    "        for x in metadata['assetsSummary'].get('dataStandard', {})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6ef52-0134-4b7a-9720-111802e7b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_publications(metadata):\n",
    "    if \"relatedResource\" in metadata:\n",
    "        for x in metadata[\"relatedResource\"]:\n",
    "            if x[\"relation\"] == \"dcite:IsDescribedBy\" and \"identifier\" in x and (\n",
    "                x[\"identifier\"].startswith(\"doi\") or x[\"identifier\"].startswith(\"https://doi\")\n",
    "            ):\n",
    "                return x[\"identifier\"]\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdc67f-7726-457c-8c33-0c2571212fe2",
   "metadata": {},
   "source": [
    "# Find DANDISets on the DANDI archive that use NWB and have an associated publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a23c9-a7d4-45eb-8086-6b9dbee92fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DandiAPIClient()\n",
    "\n",
    "dandisets = list(client.get_dandisets())\n",
    "\n",
    "neurodata_type_map = dict(\n",
    "    ecephys=[\"LFP\", \"Units\", \"ElectricalSeries\"],\n",
    "    ophys=[\"PlaneSegmentation\", \"TwoPhotonSeries\", \"ImageSegmentation\"],\n",
    "    icephys=[\n",
    "        \"PatchClampSeries\",\n",
    "        \"VoltageClampSeries\",\n",
    "        \"CurrentClampSeries\",\n",
    "        \"CurrentClampStimulusSeries\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Collect all dandiset with NWB data data\n",
    "data = defaultdict(list)\n",
    "for dandiset in tqdm(dandisets):\n",
    "    dandiset = dandiset.for_version(\"draft\")\n",
    "    identifier = dandiset.identifier\n",
    "    metadata = dandiset.get_raw_metadata()\n",
    "    \n",
    "    if not has_nwb(metadata) or not dandiset.draft_version.size:\n",
    "        continue\n",
    "    data[\"identifier\"].append(identifier)\n",
    "    data[\"name\"].append(metadata[\"name\"])\n",
    "    data[\"authors\"].append([x[\"name\"] for x in metadata.get(\"contributor\",[]) if x.get(\"includeInCitation\", False)])\n",
    "    data[\"created\"].append(dandiset.created)\n",
    "    data[\"size\"].append(pretty_size(dandiset.draft_version.size))\n",
    "    if \"species\" in metadata[\"assetsSummary\"] and len(metadata[\"assetsSummary\"][\"species\"]):\n",
    "        data[\"species\"].append(metadata[\"assetsSummary\"][\"species\"][0][\"name\"])\n",
    "    else:\n",
    "        data[\"species\"].append(np.nan)\n",
    "    \n",
    "    \n",
    "    for modality, ndtypes in neurodata_type_map.items():\n",
    "        data[modality].append(\n",
    "            any(x in ndtypes for x in metadata[\"assetsSummary\"][\"variableMeasured\"])\n",
    "        )\n",
    "    \n",
    "    data[\"numberOfSubjects\"].append(int(metadata[\"assetsSummary\"].get(\"numberOfSubjects\", 0)))\n",
    "    data[\"numberOfFiles\"].append(metadata[\"assetsSummary\"].get(\"numberOfFiles\", 0))        \n",
    "    data[\"related_pub\"].append(get_related_publications(metadata))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Update species to replace with more consisten names\n",
    "species_replacement = {\n",
    "    \"Mus musculus - House mouse\": \"House mouse\",\n",
    "    \"Rattus norvegicus - Norway rat\": \"Rat\",\n",
    "    \"Brown rat\": \"Rat\",\n",
    "    \"Rat; norway rat; rats; brown rat\": \"Rat\",\n",
    "    \"Homo sapiens - Human\": \"Human\",\n",
    "    \"Drosophila melanogaster - Fruit fly\": \"Fruit fly\",\n",
    "}\n",
    "\n",
    "for key, val in species_replacement.items():\n",
    "    df[\"species\"] = df[\"species\"].replace(key, val)\n",
    "    \n",
    "    \n",
    "# Parse the size of the dandiset and add it to the table\n",
    "def dandiset_size_to_mb(values: list[str]):\n",
    "    \"\"\"\n",
    "    Parse size strings from DANDI to translate them to an array of values in MB\n",
    "    \n",
    "    :param values: array of strings of the from `2 TB`, `5 GB`, `100 MB`\n",
    "    \n",
    "    :returns: List of ints with sizes in MB. May contain None elements for values that could not be converted\n",
    "    \"\"\"\n",
    "    outvals = [None] * len(values)\n",
    "    for i, v in enumerate(values):\n",
    "        size, unit = v.split(\" \")[0:2]\n",
    "        if unit == 'PB':\n",
    "            outvals[i] = int(size) * 1_000_000_000.\n",
    "        elif unit == 'TB':\n",
    "            outvals[i] = int(size) * 1_000_000.\n",
    "        elif unit == 'GB':\n",
    "            outvals[i] = int(size) * 1_000.\n",
    "        elif unit == 'MB':\n",
    "            outvals[i] = int(size) * 1.\n",
    "        elif unit == 'KB':\n",
    "            outvals[i] = int(size) * 0.001\n",
    "    return outvals\n",
    "\n",
    "df.insert(loc=len(df.columns), column='size in MB', value=dandiset_size_to_mb(df['size']))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de22a7e-e3d3-4515-a96a-808384078fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total size: %.2f TB\" % (df['size in MB'].sum() / 1_000_000.))\n",
    "print(\"Total number of files: %i\" % df['numberOfFiles'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c9c17-8a10-43f7-9968-b6fcb19f6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'].value_counts().plot(\n",
    "    kind='barh', \n",
    "    rot=0, \n",
    "    title=\"Number of Dandisets by Species\", \n",
    "    xlabel=\"Number of Dandisets\", \n",
    "    ylabel=\"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4f9da-1f50-4328-9776-16689923b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df[\"related_pub\"] != False]\n",
    "df2.reset_index(drop=True)\n",
    "df2[\"created\"] = df2[\"created\"].apply(lambda x: x.date())\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808cdad-051b-4fb0-8960-5e4278acea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6c4d9-ecaf-4b75-a2c2-900df9824644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel(\"dandi_w_pubs.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ea625-263e-42d3-ac06-d854c6f6303c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
